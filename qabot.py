import base64
import logging
import os
import streamlit as st
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from prepare_vector import MODELNAME, vector_db_path


load_dotenv()
api_key = os.getenv("API_KEY")
image_path = "Images/AI_img1.jpg"


def load_llm(api_key):
    if not api_key:
        raise ValueError("API key is missing. Please set your Google Gemini API key.")

    llm_model = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        api_key=api_key,
        temperature=0.2,
        convert_system_message_to_human=True
    )
    return llm_model


# Create prompt template
def create_prompt(templates):
    prompts = PromptTemplate(template=templates, input_variables=["context", "question"])
    return prompts


# Create QA chain
def create_qa_chain(prompt, llm, db):
    retriever_from_llm = MultiQueryRetriever.from_llm(
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        llm=llm
    )

    llm_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever_from_llm,
        return_source_documents=True,
        chain_type_kwargs={'prompt': prompt}
    )
    return llm_chain


# Read from vector db
def read_vectors_db(vector_db_paths):
    embedding_model = HuggingFaceEmbeddings(model_name=MODELNAME)
    db = FAISS.load_local(vector_db_paths, embedding_model, allow_dangerous_deserialization=True)
    return db


def load_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()


def main():
    st.set_page_config(page_title="Chatbot RAG", page_icon="üí¨", layout="wide")

    # ƒê·ªçc t·ª´ database vector
    vector_db = read_vectors_db(vector_db_path)
    llm_model = load_llm(api_key)

    # T·∫°o Prompt v·ªõi Chain of Thought Prompting
    template = """
    system
    B·∫°n l√† m·ªôt chatbot h·ªØu √≠ch v√† ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. 
    S·ª≠ d·ª•ng th√¥ng tin sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, h√£y tr·∫£ l·ªùi v√†o tr·ªçng t√¢m c·ªßa c√¢u h·ªèi.
    N·∫øu c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi kh√¥ng c√≥ trong ng·ªØ c·∫£nh, b·∫°n c√≥ th·ªÉ l·ªãch s·ª± n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.

    user
    C√¢u h·ªèi: {question}

    assistant
    B∆∞·ªõc 1: T√¥i s·∫Ω ph√¢n t√≠ch c√°c t√†i li·ªáu ƒë√£ truy xu·∫•t.
    Th√¥ng tin truy xu·∫•t: {context}

    B∆∞·ªõc 2: Sau khi xem x√©t c√°c t√†i li·ªáu, t√¥i s·∫Ω suy nghƒ© v·ªÅ c√¢u h·ªèi v√† c√°ch tr·∫£ l·ªùi n√≥ d·ª±a tr√™n c√°c th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.
    Suy nghƒ©: {context}

    B∆∞·ªõc 3: D·ª±a tr√™n c√°c th√¥ng tin t·ª´ t√†i li·ªáu v√† suy nghƒ© c·ªßa t√¥i, ƒë√¢y l√† c√¢u tr·∫£ l·ªùi cu·ªëi c√πng:
    """

    # T·∫°o Prompt Template cho m√¥ h√¨nh
    prompt = create_prompt(template)

    # T·∫°o chain x·ª≠ l√Ω
    llm_chain = create_qa_chain(prompt, llm_model, vector_db)

    # ƒê·ªçc v√† m√£ h√≥a h√¨nh ·∫£nh n·ªÅn
    image_base64 = load_image(image_path)

    st.markdown(
        f"""
        <style>
        .stApp {{
            background-image: url('data:image/jpg;base64,{image_base64}');
            background-size: cover;
            background-repeat: no-repeat;
            background-attachment: fixed;
            background-position: center;
        }}
        .stTextInput input {{
            color: white;  /* M√†u ch·ªØ tr·∫Øng cho h·ªôp nh·∫≠p li·ªáu */
        }}
        .stButton>button {{
            color: white;  /* M√†u ch·ªØ tr·∫Øng cho n√∫t b·∫•m */
        }}
        .stMarkdown p {{
            color: white;  /* M√†u ch·ªØ tr·∫Øng cho vƒÉn b·∫£n hi·ªÉn th·ªã */
        }}
        .stMarkdown h3 {{
            color: white;  /* M√†u ch·ªØ tr·∫Øng cho ti√™u ƒë·ªÅ */
        }}
        .stMarkdown h1 {{
            color: white;  /* M√†u ch·ªØ tr·∫Øng cho ti√™u ƒë·ªÅ ch√≠nh */
        }}
        .block-container div {{
            color: white;  /* M√†u ch·ªØ tr·∫Øng cho n·ªôi dung c·ªßa spinner */
        }}
        </style>
        """,
        unsafe_allow_html=True
    )

    st.markdown("""
    <div style="text-align: center;">
        <h1 style="color: #00FF00;">üí¨ AI assistant</h1>
    </div>
    """,
                unsafe_allow_html=True)

    # Ki·ªÉm tra xem c√≥ session n√†o l∆∞u tr·ªØ l·ªãch s·ª≠ chat kh√¥ng, n·∫øu kh√¥ng th√¨ kh·ªüi t·∫°o
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Nh·∫≠p li·ªáu cho ng∆∞·ªùi d√πng
    if user_query := st.chat_input("How can I help you?"):
        # Hi·ªÉn th·ªã tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
        st.chat_message("user").markdown(user_query)
        st.session_state.messages.append({"role": "user", "content": user_query})

        # X·ª≠ l√Ω c√¢u h·ªèi v√† l·∫•y c√¢u tr·∫£ l·ªùi t·ª´ h·ªá th·ªëng
        with st.spinner("ƒêang tr·∫£ l·ªùi..."):
            response = llm_chain.invoke({
                "query": user_query
            })
            # response = db.similarity_search(user_query, k=3)
            print(response)
            answer = response['result']
            # answer = response

        # Hi·ªÉn th·ªã ph·∫£n h·ªìi c·ªßa bot
        with st.chat_message("assistant"):
            st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})


if __name__ == "__main__":
    logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)
    main()
   